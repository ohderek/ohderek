"""
seed/mock_data.py — Generate realistic mock data for local development

Run once before starting the agent:
    python -m seed.mock_data

This creates mock_warehouse.duckdb containing three tables:
  - fct_finance__revenue       (~10,000 order line items)
  - dim_marketing__customers   (~2,000 customers)
  - fct_ops__incidents         (~500 incidents)

The columns and values match the real schema in warehouse_schema.json,
so SQL generated by the agent works identically against both DuckDB
and Snowflake — only the dialect hint in the prompt changes.

─────────────────────────────────────────────────────────────────
WHY REALISTIC DATA MATTERS
─────────────────────────────────────────────────────────────────
Mock data that's too uniform makes testing deceptive. This generator:
  - Weights region distribution realistically (us-east 35%)
  - Applies ±15% price variance per product category
  - Generates customer tiers based on actual business rules
  - Creates SLA-breaching incidents at realistic rates
  - Spreads dates across all of 2024 so date filters work

─────────────────────────────────────────────────────────────────
HOW DUCKDB BULK INSERT WORKS
─────────────────────────────────────────────────────────────────
DuckDB can ingest a pandas DataFrame directly:
    conn.execute("CREATE TABLE t AS SELECT * FROM df")

This is ~10-100x faster than row-by-row INSERT statements because
DuckDB processes the whole DataFrame in a single columnar operation.
"""

import random
from datetime import date, datetime, timedelta, timezone

import pandas as pd
import duckdb

DB_PATH = "./mock_warehouse.duckdb"

# Fixed seed = reproducible data across runs.
# If you want fresh random data, remove these lines.
random.seed(42)


# ── Date helpers ──────────────────────────────────────────────────────────────

def rand_date(start: date, end: date) -> date:
    """Return a random date between start and end (inclusive)."""
    return start + timedelta(days=random.randint(0, (end - start).days))


def rand_ts(start: date, end: date) -> datetime:
    """Return a random timezone-aware timestamp between two dates."""
    d = rand_date(start, end)
    return datetime(
        d.year, d.month, d.day,
        random.randint(0, 23), random.randint(0, 59),
        tzinfo=timezone.utc,
    )


# ── Generators ────────────────────────────────────────────────────────────────

def generate_customers(n: int = 2000) -> list[dict]:
    """
    Generate n customer records.

    Tier logic mirrors the real dbt intermediate model in dbt-snowflake-analytics:
      champion  → 10+ orders AND $1000+ LTV
      loyal     → 5+ orders AND $500+
      repeat    → 2+ orders
      one_time  → everything else
    """
    regions          = ["us-east", "us-west", "eu-west", "eu-central", "ap-southeast"]
    region_weights   = [0.35, 0.25, 0.20, 0.12, 0.08]
    channels         = ["organic", "paid_search", "social", "referral", "email"]
    categories       = ["Electronics", "Apparel", "Sports", "Home", "Health", "Appliances"]

    rows = []
    for i in range(n):
        cid        = f"cust_{i:05d}"
        signup     = rand_date(date(2021, 1, 1), date(2024, 6, 1))
        last_order = rand_date(signup, date(2024, 12, 31))

        # Realistic order frequency distribution — most customers order once
        total_orders = random.choices([1, 2, 3, 5, 10, 15], weights=[30, 25, 20, 15, 7, 3])[0]
        avg_val      = round(random.uniform(30, 500), 2)
        ltv          = round(total_orders * avg_val * random.uniform(0.8, 1.2), 2)
        days_since   = (date(2025, 1, 1) - last_order).days

        if total_orders >= 10 and ltv >= 1000:
            tier = "champion"
        elif total_orders >= 5 and ltv >= 500:
            tier = "loyal"
        elif total_orders >= 2:
            tier = "repeat"
        else:
            tier = "one_time"

        rows.append({
            "customer_id":           cid,
            "email_hash":            f"sha256_{cid}",
            "region":                random.choices(regions, weights=region_weights)[0],
            "signup_date":           signup,
            "total_orders":          total_orders,
            "lifetime_revenue":      ltv,
            "avg_order_value":       avg_val,
            "last_order_date":       last_order,
            "days_since_last_order": days_since,
            "order_tier":            tier,
            "is_churned":            days_since >= 90,
            "refund_rate":           round(random.uniform(0, 0.15), 4),
            "preferred_category":    random.choice(categories),
            "acquisition_channel":   random.choice(channels),
        })
    return rows


def generate_revenue(customers: list[dict], n: int = 10000) -> list[dict]:
    """
    Generate n order line items, linked to the customer records above.

    Each line item belongs to a customer, inheriting their region —
    this makes regional aggregations consistent with the customer dim.
    """
    categories  = ["Electronics", "Apparel", "Sports", "Home", "Health", "Appliances"]
    # Base prices per category — ±15% variance applied per order
    base_prices = {"Electronics": 250, "Apparel": 60, "Sports": 85,
                   "Home": 110, "Health": 45, "Appliances": 320}
    # Most orders complete; some cancel/refund — realistic ratio
    statuses    = ["completed"] * 7 + ["cancelled"] * 2 + ["refunded"] * 1

    rows = []
    for i in range(n):
        cust        = random.choice(customers)
        cat         = random.choice(categories)
        unit_price  = round(base_prices[cat] * random.uniform(0.85, 1.15), 2)
        qty         = random.randint(1, 4)
        gross       = round(unit_price * qty, 2)
        discount    = round(gross * random.uniform(0, 0.20), 2)
        net         = round(gross - discount, 2)
        status      = random.choice(statuses)
        is_refunded = status == "refunded"
        order_date  = rand_date(date(2024, 1, 1), date(2024, 12, 31))

        rows.append({
            "revenue_pk":        f"rev_{i:07d}",
            "order_item_id":     f"oi_{i:07d}",
            "order_id":          f"ord_{i // 3:06d}",   # ~3 items per order
            "order_date":        order_date,
            "customer_id":       cust["customer_id"],
            "product_id":        f"prod_{random.randint(1, 50):03d}",
            "product_category":  cat,
            "region":            cust["region"],
            "quantity":          qty,
            "unit_price":        unit_price,
            "gross_line_amount": gross,
            "discount_amount":   discount,
            "net_line_amount":   0.0 if is_refunded else net,
            "status":            status,
            "is_refunded":       is_refunded,
            "refund_amount":     net if is_refunded else 0.0,
        })
    return rows


def generate_incidents(n: int = 500) -> list[dict]:
    """
    Generate n incident records with realistic SLA breach rates.

    SLA thresholds mirror real DORA/ops benchmarks:
      P1 → 60 min, P2 → 240 min, P3 → 1440 min, P4 → 4320 min
    """
    services    = ["checkout", "payments", "auth", "search", "recommendations",
                   "notifications", "inventory", "data-pipeline"]
    teams       = ["platform", "commerce", "data", "security", "infrastructure"]
    severities  = ["P1"] * 1 + ["P2"] * 2 + ["P3"] * 5 + ["P4"] * 2   # realistic distribution
    sla         = {"P1": 60, "P2": 240, "P3": 1440, "P4": 4320}
    root_causes = ["infrastructure", "code_deployment", "external_dependency",
                   "configuration", "unknown"]

    rows = []
    for i in range(n):
        sev         = random.choice(severities)
        created     = rand_ts(date(2024, 1, 1), date(2024, 12, 31))
        # Resolution time: random between 5 min and 2× SLA threshold
        res_mins    = random.randint(5, sla[sev] * 2)
        is_resolved = random.random() > 0.05  # 95% are resolved
        resolved_at = (created + timedelta(minutes=res_mins)) if is_resolved else None

        rows.append({
            "incident_id":         f"inc_{i:05d}",
            "created_at":          created,
            "resolved_at":         resolved_at,
            "severity":            sev,
            "service":             random.choice(services),
            "team":                random.choice(teams),
            "resolution_minutes":  res_mins if is_resolved else None,
            "is_resolved":         is_resolved,
            "root_cause_category": random.choice(root_causes),
            "mttr_minutes":        res_mins if is_resolved else None,
            "breach_sla":          is_resolved and res_mins > sla[sev],
        })
    return rows


# ── Main ──────────────────────────────────────────────────────────────────────

def seed():
    """Create mock_warehouse.duckdb and populate all three tables."""
    print(f"[seed] Creating {DB_PATH}")
    conn = duckdb.connect(DB_PATH)

    print("[seed] Generating data...")
    customers = generate_customers(2_000)
    revenue   = generate_revenue(customers, 10_000)
    incidents = generate_incidents(500)

    datasets = [
        ("dim_marketing__customers", customers),
        ("fct_finance__revenue",     revenue),
        ("fct_ops__incidents",       incidents),
    ]

    for table_name, rows in datasets:
        df = pd.DataFrame(rows)
        conn.execute(f"DROP TABLE IF EXISTS {table_name}")
        # CREATE TABLE AS SELECT FROM df uses DuckDB's native pandas integration —
        # no type mapping needed, it infers column types from the DataFrame dtypes.
        conn.execute(f"CREATE TABLE {table_name} AS SELECT * FROM df")
        count = conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
        print(f"[seed]   {table_name}: {count:,} rows")

    conn.close()
    print(f"\n[seed] Done. Next steps:")
    print(f"  make start   ← launch the API server")
    print(f"  make test    ← run a sample query")
    print(f"  open http://localhost:8000/docs  ← interactive API explorer")


if __name__ == "__main__":
    seed()
